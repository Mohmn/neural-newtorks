{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A= 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A,cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A,cache\n",
    "\n",
    "#Dz= dA*differnrential of A\n",
    "\n",
    "def sigmoid_backward(dA,cache): #cache = Z\n",
    "    Z = cache\n",
    "    dfA = 1/(1+np.exp(-Z))\n",
    "    dZ = dA* dfA*(1-dfA)\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA,cache):\n",
    "    Z=cache\n",
    "    dfA = np.where(Z>=0,1,0)\n",
    "    dZ = dA * dfA\n",
    "    \n",
    "    assert(dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.5 1. ]\n",
      " [2.  3.  4. ]]\n"
     ]
    }
   ],
   "source": [
    "b = np.array(([2,1,2],[4,6,8]))\n",
    "b = b*(1/2)*(2/2)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(layer_dimn):\n",
    "    L = len(layer_dimn)\n",
    "   \n",
    "    parameters = {}\n",
    "    for i in range(1,L):\n",
    "        parameters[\"W\"+str(i)] = np.random.randn(layer_dimn[i],layer_dimn[i-1])*np.sqrt((2/layer_dimn[i-1]))\n",
    "        parameters[\"b\" + str(i)] = np.zeros((layer_dimn[i],1))\n",
    "        \n",
    "        assert(parameters['W' + str(i)].shape == (layer_dimn[i], layer_dimn[i-1]))\n",
    "        assert(parameters['b' + str(i)].shape == (layer_dimn[i], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = (4, 5)\n",
      "b1 = (4, 1)\n",
      "W2 = (3, 4)\n",
      "b2 = (3, 1)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_weights([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"].shape))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"].shape))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"].shape))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A,W,b):\n",
    "\n",
    "\tZ=np.dot(W,A) + b\n",
    "\n",
    "\tassert(Z.shape ==(W.shape[0],A.shape[1]))\n",
    "\n",
    "\tcache = (A,W,b)\n",
    "\treturn Z,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.26295336 -1.23429988]]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([[ 1.62434536 ,-0.61175641],\n",
    " [-0.52817175 ,-1.07296862],\n",
    " [ 0.86540763 ,-2.3015387 ]] )\n",
    "W = np.array([[ 1.74481176, -0.7612069  , 0.3190391 ]])\n",
    "b=np.array( [[-0.24937038]])\n",
    "Z,c=linear_forward(A,W,b)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev,W,b,activation):\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z,linear_cache = linear_forward(A_prev,W,b)\n",
    "        A,activation_cache = sigmoid(Z)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        Z,linear_cache = linear_forward(A_prev,W,b)\n",
    "        A,activation_cache = relu(Z)\n",
    "    \n",
    "    assert(A.shape== (W.shape[0],A_prev.shape[1]))\n",
    "    \n",
    "    cache =(linear_cache,activation_cache)\n",
    "    \n",
    "    return A,cache\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896134 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev = np.array([[-0.41675785, -0.05626683],\n",
    " [-2.1361961 ,1.64027081],\n",
    " [-1.79343559 ,-0.84174737]])\n",
    "W = np.array([[ 0.50288142, -1.24528809, -1.05795222]])\n",
    "b=np.array([[-0.90900761]])\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X,parameters):\n",
    "    \n",
    "    \n",
    "    caches = []\n",
    "    L = len(parameters) // 2\n",
    "    A=X\n",
    "    \n",
    "    for i in range(1,L):\n",
    "        A_prev = A\n",
    "        A,cache = linear_activation_forward(A_prev,parameters[\"W\" + str(i)],parameters[\"b\"+str(i)],activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    #last layer sigmoid\n",
    "    AL,cache = linear_activation_forward(A,parameters[\"W\" + str(L)],parameters[\"b\"+str(L)],activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    # AL -> last activation func( sig)\n",
    "    #single output\n",
    "   \n",
    "   # assert(AL.shape == (1,X.shape[1]))\n",
    "    \n",
    "    return AL,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[-0.31178367 , 0.72900392,  0.21782079, -0.8990918 ],\n",
    " [-2.48678065 , 0.91325152 , 1.12706373, -1.51409323],\n",
    " [ 1.63929108 ,-0.4298936  , 2.63128056 , 0.60182225],\n",
    " [-0.33588161 , 1.23773784  ,0.11112817  ,0.12915125],\n",
    " [ 0.07612761 ,-0.15512816  ,0.63422534 , 0.810655  ]])\n",
    "parameters={'W2': np.array([[-0.12673638, -1.36861282,  1.21848065, -0.85750144],\n",
    "       [-0.56147088, -1.0335199 ,  0.35877096,  1.07368134],\n",
    "       [-0.37550472,  0.39636757, -0.47144628,  2.33660781]]), 'b2': np.array([[ 1.50278553],\n",
    "       [-0.59545972],\n",
    "       [ 0.52834106]]), 'b1': np.array([[ 1.38503523],\n",
    "       [-0.51962709],\n",
    "       [-0.78015214],\n",
    "       [ 0.95560959]]), 'b3': np.array([[-0.16236698]]), 'W1': np.array([[ 0.35480861,  1.81259031, -1.3564758 , -0.46363197,  0.82465384],\n",
    "       [-1.17643148,  1.56448966,  0.71270509, -0.1810066 ,  0.53419953],\n",
    "       [-0.58661296, -1.48185327,  0.85724762,  0.94309899,  0.11444143],\n",
    "       [-0.02195668, -2.12714455, -0.83440747, -0.46550831,  0.23371059]]), 'W3':np. array([[ 0.9398248 ,  0.42628539, -0.75815703]])}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4)\n",
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "AL,caches = forward_propagation(X,parameters)\n",
    "print(AL.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL,Y):\n",
    "\n",
    "    m =Y.shape[1]\n",
    "    \n",
    "    cost = np.sum(np.multiply(Y,np.log(AL)),axis=1,keepdims=True) + np.sum(np.multiply(1-Y,np.log(1-AL)),axis=1,keepdims=True)\n",
    "    cost=np.sum(cost)\n",
    "    cost = cost/-m  \n",
    "    \n",
    "    cost = np.squeeze(cost) #(will turn [[13]] -> 13)\n",
    "    \n",
    "    assert(cost.shape == ())\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(AL,Y,parameters,lambdaa):\n",
    "    if lambdaa == 0: \n",
    "       \n",
    "        \n",
    "        return compute_cost(AL,Y)\n",
    "    else:\n",
    "        \n",
    "        m = Y.shape[1]\n",
    "        reg_cost = 0\n",
    "        for i in range(len(parameters)//2):\n",
    "            reg_cost += np.sum(np.square(parameters[\"W\"+str(i+1)]))\n",
    "\n",
    "        return (compute_cost(AL,Y)) + ((reg_cost*(1/m)*(lambdaa/2)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.414931599615397\n"
     ]
    }
   ],
   "source": [
    "Y= np.array([[1, 1, 1]])\n",
    "AL = np.array([[ 0.8,  0.9,  0.4]])\n",
    "print(compute_cost(AL,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ,cache):\n",
    "    \n",
    "    A_prev,W,b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = np.dot(dZ,A_prev.T)/m\n",
    "    db = np.sum(dZ,axis=1,keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev,dW,db\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dZ = np.array([[ 1.62434536, -0.61175641]])\n",
    "linear_cache = (np.array([[-0.52817175, -1.07296862],\n",
    "       [ 0.86540763, -2.3015387 ],\n",
    "       [ 1.74481176, -0.7612069 ]]),np.array([[ 0.3190391 , -0.24937038,  1.46210794]]),np.array([[-2.06014071]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506362  0.15255393]\n",
      " [ 2.37496825 -0.8944539 ]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992504]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA,cache,activation):\n",
    "    \n",
    "    linear_cache,activation_cache = cache\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev,dW,db = linear_backward(dZ,linear_cache)\n",
    "        \n",
    "    elif activation == \"relu\":\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev,dW,db = linear_backward(dZ,linear_cache)\n",
    "    \n",
    "    return dA_prev,dW,db \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAL = np.array([[-0.41675785,-0.05626683]])\n",
    "linear_activation_cache = ((np.array([[-2.1361961 ,  1.64027081],\n",
    "       [-1.79343559, -0.84174737],\n",
    "       [ 0.50288142, -1.24528809]]), np.array([[-1.05795222, -0.90900761,  0.55145404]]), \n",
    "        np.array([[ 2.29220801]])), np.array([[ 0.04153939, -1.11792545]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.0110534 ]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576155]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989  0.        ]\n",
      " [ 0.37883606  0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513825  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propogation(AL,Y,caches):\n",
    "    grads={}\n",
    "    #Y = Y.reshape(AL.shape)\n",
    "    dAL = -((np.divide(Y,AL)) - (np.divide(1-Y,1-AL)))\n",
    "    \n",
    "    L = len(caches)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)],grads[\"dW\" + str(L)],grads[\"db\" + str(L)] = linear_activation_backward(dAL,current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    for i in reversed(range(L-1)):\n",
    "        current_cache = caches[i]\n",
    "        dA_prev_temp,dW_temp,db_temp = linear_activation_backward(grads[\"dA\" + str(i+1)],current_cache,activation=\"relu\")\n",
    "        grads[\"dA\" + str(i)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(i+1)] = dW_temp\n",
    "        grads[\"db\" + str(i+1)] = db_temp\n",
    "    \n",
    "    return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 4\n",
      "3 2\n",
      "2 1\n",
      "1 0\n"
     ]
    }
   ],
   "source": [
    "l=4\n",
    "print(l-1,l)\n",
    "for i in reversed(range(l-1)):\n",
    "    print(i+1,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updating_weights_with_reg(parameters,grads,lamb_m):\n",
    "    \n",
    "    L = len(grads) - len(parameters)\n",
    "    \n",
    "    for i in range(L):\n",
    "        grads[\"dW\" + str(i+1)] = grads[\"dW\" + str(i+1)] + ((lamb_m)* parameters[\"W\" + str(i+1)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dA1': array([[ 0.12913162, -0.44014127],\n",
      "       [-0.14175655,  0.48317296],\n",
      "       [ 0.01663708, -0.05670697]]), 'dW2': array([[-0.39202432, -0.13325855, -0.04601089]]), 'db2': array([[0.15187861]]), 'dA0': array([[ 0.        ,  0.52257901],\n",
      "       [ 0.        , -0.3269206 ],\n",
      "       [ 0.        , -0.32070404],\n",
      "       [ 0.        , -0.74079187]]), 'dW1': array([[0.41010002, 0.07807203, 0.13798444, 0.10502167],\n",
      "       [0.        , 0.        , 0.        , 0.        ],\n",
      "       [0.05283652, 0.01005865, 0.01777766, 0.0135308 ]]), 'db1': array([[-0.22007063],\n",
      "       [ 0.        ],\n",
      "       [-0.02835349]])}\n"
     ]
    }
   ],
   "source": [
    "AL = np.array([[ 1.78862847 , 0.43650985]])\n",
    "y = np.array([[1 ,0]])\n",
    "caches = (((np.array([[ 0.09649747, -1.8634927 ],\n",
    "       [-0.2773882 , -0.35475898],\n",
    "       [-0.08274148, -0.62700068],\n",
    "       [-0.04381817, -0.47721803]]), np.array([[-1.31386475,  0.88462238,  0.88131804,  1.70957306],\n",
    "       [ 0.05003364, -0.40467741, -0.54535995, -1.54647732],\n",
    "       [ 0.98236743, -1.10106763, -1.18504653, -0.2056499 ]]), np.array([[ 1.48614836],\n",
    "       [ 0.23671627],\n",
    "       [-1.02378514]])), np.array([[-0.7129932 ,  0.62524497],\n",
    "       [-0.16051336, -0.76883635],\n",
    "       [-0.23003072,  0.74505627]])), ((np.array([[ 1.97611078, -1.24412333],\n",
    "       [-0.62641691, -0.80376609],\n",
    "       [-2.41908317, -0.92379202]]), np.array([[-1.02387576,  1.12397796, -0.13191423]]), np.array([[-1.62328545]])), np.array([[ 0.64667545, -0.35627076]])))\n",
    "grads = backward_propogation(AL,y,caches)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(parameters,grads,learning_rate):\n",
    "    L = len(parameters) //2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate*grads[\"dW\"+str(l+1)])\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate*grads[\"db\"+str(l+1)])\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_adam_prm(parameters):\n",
    "    v,s= {},{}\n",
    "    L = len(parameters)//2\n",
    "    for i in range(L):\n",
    "        v[\"dW\" + str(i+1)] = np.zeros(parameters[\"W\" + str(i+1)].shape)\n",
    "        v[\"db\" + str(i+1)] = np.zeros(parameters[\"b\" + str(i+1)].shape)\n",
    "        s[\"dW\" + str(i+1)] = np.zeros(parameters[\"W\" + str(i+1)].shape)\n",
    "        s[\"db\" + str(i+1)] = np.zeros(parameters[\"b\" + str(i+1)].shape)\n",
    "        \n",
    "    return v,s\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_optimizer(parameters,grads,v,s,t,learning_rate=0.01,beta1=0.9,beta2=0.999,epsilon=1e-8):\n",
    "    #print(learning_rate)\n",
    "    v_corrected,s_corrected={},{}\n",
    "    L = len(parameters)//2\n",
    "    for i in range(L):\n",
    "        v[\"dW\" + str(i+1)] = beta1 * v[\"dW\" + str(i+1)] + (1-beta1)*grads[\"dW\" + str(i+1)]\n",
    "        v[\"db\" + str(i+1)] = beta1 * v[\"db\" + str(i+1)] + (1-beta1)*grads[\"db\" + str(i+1)]\n",
    "        \n",
    "        v_corrected[\"dW\" + str(i+1)] = v[\"dW\" + str(i+1)]/(1-(beta1**t))\n",
    "        v_corrected[\"db\" + str(i+1)] = v[\"db\" + str(i+1)]/(1-(beta1**t))\n",
    "        \n",
    "        s[\"dW\" + str(i+1)] = beta2 * s[\"dW\" + str(i+1)] + (1-beta2)*(grads[\"dW\" + str(i+1)]**2)\n",
    "        s[\"db\" + str(i+1)] = beta2 * s[\"db\" + str(i+1)] + (1-beta2)*(grads[\"db\" + str(i+1)]**2)\n",
    "        \n",
    "        s_corrected[\"dW\" + str(i+1)] = s[\"dW\" + str(i+1)]/(1-(beta2**t))\n",
    "        s_corrected[\"db\" + str(i+1)] = s[\"db\" + str(i+1)]/(1-(beta2**t))\n",
    "        \n",
    "        parameters[\"W\" + str(i+1)] =parameters[\"W\" + str(i+1)] - (learning_rate*(v_corrected[\"dW\" + str(i+1)]/(np.sqrt(s_corrected[\"dW\" + str(i+1)])+epsilon)))\n",
    "        parameters[\"b\" + str(i+1)] =parameters[\"b\" + str(i+1)] - (learning_rate*(v_corrected[\"db\" + str(i+1)]/(np.sqrt(s_corrected[\"db\" + str(i+1)])+epsilon)))\n",
    "    \n",
    "                                                                  \n",
    "    return parameters,v,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={'W1': np.array([[ 1.62434536, -0.61175641, -0.52817175],\n",
    "       [-1.07296862,  0.86540763, -2.3015387 ]]), 'b1': np.array([[ 1.74481176],\n",
    "       [-0.7612069 ]]), 'W2': np.array([[ 0.3190391 , -0.24937038,  1.46210794],\n",
    "       [-2.06014071, -0.3224172 , -0.38405435],\n",
    "       [ 1.13376944, -1.09989127, -0.17242821]]), 'b2': np.array([[-0.87785842],\n",
    "       [ 0.04221375],\n",
    "       [ 0.58281521]])}\n",
    "\n",
    "grads = {'dW1': np.array([[-1.10061918,  1.14472371,  0.90159072],\n",
    "       [ 0.50249434,  0.90085595, -0.68372786]]), 'db1': np.array([[-0.12289023],\n",
    "       [-0.93576943]]), 'dW2': np.array([[-0.26788808,  0.53035547, -0.69166075],\n",
    "       [-0.39675353, -0.6871727 , -0.84520564],\n",
    "       [-0.67124613, -0.0126646 , -1.11731035]]), 'db2': np.array([[ 0.2344157 ],\n",
    "       [ 1.65980218],\n",
    "       [ 0.74204416]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 1.6392281  -0.62663915 -0.54305449]\n",
      " [-1.08785136  0.85052489 -2.28665596]]\n",
      "b1 = [[ 1.75969449]\n",
      " [-0.74632416]]\n",
      "W2 = [[ 0.33392184 -0.26425312  1.47699068]\n",
      " [-2.04525797 -0.30753446 -0.36917161]\n",
      " [ 1.14865218 -1.08500855 -0.15754547]]\n",
      "b2 = [[-0.89274116]\n",
      " [ 0.02733101]\n",
      " [ 0.56793247]]\n",
      "v[\"dW1\"] = [[-0.11006192  0.11447237  0.09015907]\n",
      " [ 0.05024943  0.09008559 -0.06837279]]\n",
      "v[\"db1\"] = [[-0.01228902]\n",
      " [-0.09357694]]\n",
      "v[\"dW2\"] = [[-0.02678881  0.05303555 -0.06916607]\n",
      " [-0.03967535 -0.06871727 -0.08452056]\n",
      " [-0.06712461 -0.00126646 -0.11173103]]\n",
      "v[\"db2\"] = [[0.02344157]\n",
      " [0.16598022]\n",
      " [0.07420442]]\n",
      "s[\"dW1\"] = [[0.00121136 0.00131039 0.00081287]\n",
      " [0.0002525  0.00081154 0.00046748]]\n",
      "s[\"db1\"] = [[1.51020086e-05]\n",
      " [8.75664426e-04]]\n",
      "s[\"dW2\"] = [[7.17640234e-05 2.81276925e-04 4.78394593e-04]\n",
      " [1.57413364e-04 4.72206320e-04 7.14372574e-04]\n",
      " [4.50571367e-04 1.60392093e-07 1.24838242e-03]]\n",
      "s[\"db2\"] = [[5.49507204e-05]\n",
      " [2.75494328e-03]\n",
      " [5.50629535e-04]]\n"
     ]
    }
   ],
   "source": [
    "parameters,v,s = adam_optimizer(parameters, grads, t=2)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))\n",
    "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))\n",
    "print(\"s[\\\"dW1\\\"] = \" + str(s[\"dW1\"]))\n",
    "print(\"s[\\\"db1\\\"] = \" + str(s[\"db1\"]))\n",
    "print(\"s[\\\"dW2\\\"] = \" + str(s[\"dW2\"]))\n",
    "print(\"s[\\\"db2\\\"] = \" + str(s[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X,Y,parameters):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    problty,cache = forward_propagation(X,parameters)\n",
    "\n",
    "    p = np.argmax(problty,axis=0)+1\n",
    "    p=p.reshape(1,p.shape[0])\n",
    "    print(\"prediction accuracy\" + \" \" + str(np.sum((p == Y))/m))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def data(path):\n",
    "\n",
    "    os.chdir(path)\n",
    "    #os.chdir(\"cats\")\n",
    "    aa=[\"cats\",\"dogs\"]\n",
    "    training_data=[]\n",
    "\n",
    "    #print(X)\n",
    "    for a in aa:\n",
    "        path = os.path.join(os.getcwd(),a)\n",
    "        ind = aa.index(a) + 1\n",
    "        print(path)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "\n",
    "                image  = Image.open(os.path.join(path,img))\n",
    "                image = image.convert(\"L\")\n",
    "                image =image.resize((80,80))\n",
    "                \n",
    "                image = np.array(image)\n",
    "                training_data.append([image,ind])\n",
    "            except IOError:\n",
    "                pass\n",
    "    \n",
    "    random.shuffle(training_data)\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for x,y in training_data:\n",
    "        X.append(x)\n",
    "        Y.append(y)\n",
    "    X=np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    X=X.reshape(X.shape[0], -1).T\n",
    "    \n",
    "    Y=Y.reshape(1,Y.shape[0])\n",
    "  \n",
    "\n",
    "          \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPY\\Desktop\n",
      "C:\\Users\\LAPY\\Desktop\\cat-and-dog\\training_set\\cats\n",
      "C:\\Users\\LAPY\\Desktop\\cat-and-dog\\training_set\\dogs\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"../../../\")\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "X_train,Y_train = data(\"cat-and-dog/training_set\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPY\\Desktop\n",
      "(6400, 8005) (1, 8005)\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"../../\")\n",
    "print(os.getcwd())\n",
    "print(X_train.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 2 2 2 2 2 2 2 1 2 1 2 1 2 2 2 2 1 1 1 1 1 2 1 1 1 1 2 1 2 1 1 1 1 2\n",
      "  1 2 2 1 2 1 2 1 1 1 1 2 1 1 2 1 2 2 2 2 2 2 1 1 1 1 2 1 2 2 2 2 1 2 2 1\n",
      "  1 1 2 2 1 1 2 2 1 2 1 1 1 2 2 2 2 1]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1\n",
      "  0 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 1 0 1 1 1 1 0 1 1 0\n",
      "  0 0 1 1 0 0 1 1 0 1 0 0 0 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "a = np.where(Y_test ==2,1,0)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPY\\Desktop\\cat-and-dog\\test_set\\test_set\\cats\n",
      "C:\\Users\\LAPY\\Desktop\\cat-and-dog\\test_set\\test_set\\dogs\n",
      "(6400, 2023) (1, 2023)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "X_test,Y_test = data(\"cat-and-dog/test_set/test_set\")\n",
    "print(X_test.shape,Y_test.shape)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.61960784 0.38039216 0.65098039 ... 0.7372549  0.81568627 0.14117647]\n",
      " [0.63921569 0.41960784 0.41176471 ... 0.75294118 0.74117647 0.1254902 ]\n",
      " [0.65490196 0.41960784 0.33333333 ... 0.72941176 0.84313725 0.14117647]\n",
      " ...\n",
      " [0.98823529 0.61568627 0.38431373 ... 0.58431373 0.29019608 0.7254902 ]\n",
      " [0.98823529 0.61176471 0.3254902  ... 0.58823529 0.52156863 0.71764706]\n",
      " [0.98823529 0.63137255 0.34117647 ... 0.60392157 0.30980392 0.70588235]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "\n",
    "X_test = X_test/255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_vec(dic):\n",
    "    keys = list(dic.keys())\n",
    "    for i in range(len(keys)):\n",
    "        if i == 0:\n",
    "            vec = np.reshape(dic[str(keys[i])],(dic[str(keys[i])].shape[0]*dic[str(keys[i])].shape[1],1))\n",
    "        else:\n",
    "            other_vec =  np.reshape(dic[str(keys[i])],(dic[str(keys[i])].shape[0]*dic[str(keys[i])].shape[1],1)) \n",
    "            vec = np.concatenate((vec,other_vec))\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_to_dict(dic,vec):\n",
    "    dc = {}\n",
    "    keys = list(dic.keys())\n",
    "    l=0\n",
    "    for i in range(len(keys)):\n",
    "        \n",
    "        dc[str(keys[i])] = vec[l:((dic[str(keys[i])].shape[0]*dic[str(keys[i])].shape[1]) + l)]\n",
    "        dc[str(keys[i])] = np.reshape(dc[str(keys[i])],(dic[str(keys[i])].shape[0],dic[str(keys[i])].shape[1]))\n",
    "       \n",
    "        l += dic[str(keys[i])].shape[0]*dic[str(keys[i])].shape[1]\n",
    "        \n",
    "    return dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradients_to_vector(gradients):\n",
    "    \"\"\"\n",
    "    Roll all our gradients dictionary into a single vector satisfying our specific required shape.\n",
    "    \"\"\"\n",
    "    \n",
    "    count = 0\n",
    "    for key in [\"dW1\", \"db1\", \"dW2\", \"db2\", \"dW3\", \"db3\"]:\n",
    "        # flatten parameter\n",
    "        new_vector = np.reshape(gradients[key], (-1,1))\n",
    "        \n",
    "        if count == 0:\n",
    "            theta = new_vector\n",
    "        else:\n",
    "            theta = np.concatenate((theta, new_vector), axis=0)\n",
    "        count = count + 1\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_checking(parameters,gradients,X,Y,epsilon = 1e-7):\n",
    "    \n",
    "    param_vec = dict_to_vec(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_param = param_vec.shape[0]\n",
    "    J_plus = np.zeros((num_param,1))\n",
    "    J_minus = np.zeros((num_param,1))\n",
    "    grad_approx = np.zeros((num_param,1))\n",
    "    \n",
    "    for i in range(num_param):\n",
    "        \n",
    "        theta_plus = np.copy(param_vec)\n",
    "        theta_plus[i][0] += epsilon\n",
    "        AL,_ = forward_propagation(X,vec_to_dict(parameters,theta_plus))\n",
    "        J_plus[i] = compute_cost(AL,Y)\n",
    "        \n",
    "        theta_minus = np.copy(param_vec)\n",
    "        theta_minus[i][0] -= epsilon\n",
    "        AL,_ = forward_propagation(X,vec_to_dict(parameters,theta_minus))\n",
    "        J_minus[i] = compute_cost(AL,Y)\n",
    "        \n",
    "        grad_approx[i] = (J_plus[i] - J_minus[i])/(2*epsilon)\n",
    "        \n",
    "    numerator = np.linalg.norm(grad - grad_approx)\n",
    "    denomenator = np.linalg.norm(grad) + np.linalg.norm(grad_approx)\n",
    "    \n",
    "    difference = numerator/denomenator\n",
    "    \n",
    "    if difference > 2e-7:\n",
    "        print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mini_batches(X,Y,mini_batch_size):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    num_complete_minibatches = m//mini_batch_size\n",
    "    mini_batches = []\n",
    "    \n",
    "    for i in range(num_complete_minibatches):\n",
    "        mini_batch_X = X[:, i*mini_batch_size:(i+1)*mini_batch_size ] \n",
    "        mini_batch_Y = Y[:, i*mini_batch_size:(i+1)*mini_batch_size ]\n",
    "        \n",
    "        mini_batch = (mini_batch_X,mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "    if m%mini_batch_size!=0:\n",
    "        mini_batch_X = X[:, num_complete_minibatches*mini_batch_size:m ] \n",
    "        mini_batch_Y = Y[:, num_complete_minibatches*mini_batch_size:m ]\n",
    "        \n",
    "        mini_batch = (mini_batch_X,mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([[ 1,  4,  7, 10, 13, 16, 19, 22],\n",
      "       [ 2,  5,  8, 11, 14, 17, 20, 23],\n",
      "       [ 3,  6,  9, 12, 15, 18, 21, 24]]), array([[1, 1, 0, 1, 1, 0, 0, 0]])), (array([[25, 28],\n",
      "       [26, 29],\n",
      "       [27, 30]]), array([[1, 0]]))]\n"
     ]
    }
   ],
   "source": [
    "a = np.array(([1,2,3],[4,5,6],[7,8,9],[10,11,12],[13,14,15],[16,17,18],[19,20,21],[22,23,24],[25,26,27],[28,29,30]))\n",
    "a=a.T\n",
    "b =np.array(([1],[1],[0],[1],[1],[0],[0],[0],[1],[0]))\n",
    "b=b.T\n",
    "c=init_mini_batches(a,b,8)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X,Y,layer_dims,num_iters,learning_rate,print_cost):\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    parameters = initialize_weights(layer_dims)\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "   \n",
    "    \n",
    "    for i in range(0,num_iters):\n",
    "        A1,cache1 = linear_activation_forward(X,W1,b1,activation = \"relu\")\n",
    "        A2,cache2 = linear_activation_forward(A1,W2,b2,activation = \"sigmoid\")\n",
    "        \n",
    "        cost = compute_cost(A2,Y)\n",
    "        print(cost)\n",
    "        dA2  = -((np.divide(Y,A2)) - (np.divide(1-Y,1-A2)))\n",
    "       \n",
    "        break\n",
    "        dA1,dW2,db2 = linear_activation_backward(dA2,cache2,activation=\"sigmoid\")\n",
    "        dA0,dW1,db1 = linear_activation_backward(dA1,cache1,activation=\"relu\")\n",
    "        \n",
    "        grads[\"dW1\"] = dW1 \n",
    "        grads[\"db1\"] = db1\n",
    "        grads[\"dW2\"] = dW2 \n",
    "        grads[\"db2\"] = db2\n",
    "        \n",
    "        parameters = gradient_descent(parameters,grads,learning_rate)\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn(X,Y,layer_dims,num_iters,learning_rate,print_cost):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    grads = {}\n",
    "    costs = []\n",
    "    \n",
    "    y=np.array(([1,2],))\n",
    "    \n",
    "   # mini_batches = init_mini_batches(X,Y,1024)\n",
    "    \n",
    "    parameters = initialize_weights(layer_dims)\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    for i in range(0,num_iters):\n",
    "        \n",
    "        shuffle(X,Y,i+1)\n",
    "        mini_batches = init_mini_batches(X,Y,64)\n",
    "            \n",
    "        cost=0\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            \n",
    "            \n",
    "            \n",
    "            (minibatch_X,minibatch_Y) = mini_batch\n",
    "            n=minibatch_Y.shape[1]\n",
    "        \n",
    "            z1 = np.dot(W1,minibatch_X) + b1\n",
    "\n",
    "            A1,_ = relu(z1)\n",
    "\n",
    "            z2 = np.dot(W2,A1) + b2\n",
    "\n",
    "            A2,_ = sigmoid(z2)\n",
    "\n",
    "            reg = np.sum(np.square(W1)) + np.sum(np.square(W2)) \n",
    "\n",
    "            #multioutput y_o\n",
    "\n",
    "            y_o = np.where(y == minibatch_Y.T,1,0)\n",
    "            y_o = y_o.T\n",
    "\n",
    "            cost += compute_cost(A2,y_o) #+ (reg*(lamd/2)*(1/m))\n",
    "\n",
    "            dA2  = -((np.divide(y_o,A2)) - (np.divide(1-y_o,1-A2)))\n",
    "        \n",
    "            dz2 = sigmoid_backward(dA2,z2)\n",
    "\n",
    "            dW2 = (np.dot(dz2,A1.T)/n) #+ ((lamd/m)*W2)\n",
    "\n",
    "            db2 = 1./n * (np.sum(dz2, axis=1, keepdims = True))\n",
    "\n",
    "            dA1 = np.dot(W2.T,dz2)\n",
    "\n",
    "            dz1 = relu_backward(dA1,z1)\n",
    "\n",
    "            dW1 = (np.dot(dz1,minibatch_X.T)/n) #+ ((lamd/m)*W1)\n",
    "            db1 = 1./n *( np.sum(dz1 , axis=1, keepdims = True))\n",
    "\n",
    "\n",
    "            grads[\"dW1\"] = dW1 \n",
    "            grads[\"db1\"] = db1\n",
    "            grads[\"dW2\"] = dW2 \n",
    "            grads[\"db2\"] = db2\n",
    "\n",
    "            parameters = gradient_descent(parameters,grads,learning_rate)\n",
    "            W1 = parameters[\"W1\"]\n",
    "            b1 = parameters[\"b1\"]\n",
    "            W2 = parameters[\"W2\"]\n",
    "            b2 = parameters[\"b2\"]\n",
    "        \n",
    "        \n",
    "        if print_cost and i %25 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost/len(mini_batches))))\n",
    "        if print_cost and i % 1 == 0:\n",
    "            costs.append(cost/len(mini_batches))\n",
    "    \n",
    "    \n",
    "    \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(a,b,seed):\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(a)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layered_network(X,Y,layer_dims,num_iters,learning_rate,print_cost,lambdaa):\n",
    "    m = X.shape[1]\n",
    "    costs = []\n",
    "    lamd_m = (lambdaa/m)\n",
    "    parameters = initialize_weights(layer_dims)\n",
    "    v,s = init_adam_prm(parameters)\n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    \n",
    "    y=np.array(([1,2],))\n",
    "    t=0\n",
    "    for i in range(0,num_iters):\n",
    "\n",
    "        try:\n",
    "            \n",
    "            shuffle(X,Y,i+1)\n",
    "            mini_batches = init_mini_batches(X,Y,64)\n",
    "            \n",
    "            cost=0\n",
    "\n",
    "            for mini_batch in mini_batches:\n",
    "\n",
    "                (minibatch_X,minibatch_Y) = mini_batch\n",
    "                #shuffle(minibatch_X,minibatch_Y,)\n",
    "                #n=minibatch_Y.shape[0]\n",
    "\n",
    "                AL,caches = forward_propagation(minibatch_X,parameters)\n",
    "                y_o = np.where(y == minibatch_Y.T,1,0)\n",
    "                y_o = y_o.T\n",
    "                #print(AL)\n",
    "                #break\n",
    "                cost += compute_cost(AL,y_o)\n",
    "\n",
    "                grads = backward_propogation(AL,y_o,caches)\n",
    "\n",
    "                #updating_weights_with_reg(parameters,grads,lamd_m)\n",
    "                #for I in range(L):\n",
    "                    #grads[\"dW\" + str(I+1)] = grads[\"dW\" + str(I+1)] #+ ((lamd_m)* parameters[\"W\" + str(I+1)])\n",
    "                t=t+1\n",
    "                parameters,v,s = adam_optimizer(parameters,grads,v,s,t,learning_rate=0.000055)\n",
    "                #parameters = gradient_descent(parameters,grads,0.0001)\n",
    "\n",
    "            if print_cost and i % 20 == 0:\n",
    "                \n",
    "                print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)/len(mini_batches)))\n",
    "            if print_cost and i % 1 == 0:\n",
    "                costs.append(cost/len(mini_batches))\n",
    "        except KeyboardInterrupt:\n",
    "            plt.plot(np.squeeze(costs))\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "            return parameters \n",
    "            \n",
    "\n",
    "\n",
    "    \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.show()\n",
    "        \n",
    "        \n",
    "    return parameters    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.4068378108898734\n",
      "Cost after iteration 25: 1.3925745734402146\n",
      "Cost after iteration 50: 1.386338302527319\n",
      "Cost after iteration 75: 1.3851511817962963\n",
      "Cost after iteration 100: 1.3910320012052195\n",
      "Cost after iteration 125: 1.3838821530054357\n",
      "Cost after iteration 150: 1.3825093885287194\n",
      "Cost after iteration 175: 1.3817771129906287\n",
      "Cost after iteration 200: 1.3804529188575896\n",
      "Cost after iteration 225: 1.3819510678321134\n",
      "Cost after iteration 250: 1.383977142555409\n",
      "Cost after iteration 275: 1.3806043485891208\n",
      "Cost after iteration 300: 1.3815919525288654\n",
      "Cost after iteration 325: 1.380194672016705\n",
      "Cost after iteration 350: 1.3787663173103986\n",
      "Cost after iteration 375: 1.3859551318896257\n",
      "Cost after iteration 400: 1.3750403573062695\n",
      "Cost after iteration 425: 1.3843469386146359\n",
      "Cost after iteration 450: 1.3792777667651164\n",
      "Cost after iteration 475: 1.3808534713805294\n",
      "Cost after iteration 500: 1.3816538925635609\n",
      "Cost after iteration 525: 1.3802317604626404\n",
      "Cost after iteration 550: 1.383286406585116\n",
      "Cost after iteration 575: 1.3794216260836254\n",
      "Cost after iteration 600: 1.3824412042810943\n",
      "Cost after iteration 625: 1.3798162531988227\n",
      "Cost after iteration 650: 1.3805375027337965\n",
      "Cost after iteration 675: 1.3789081743493175\n",
      "Cost after iteration 700: 1.382289642778252\n",
      "Cost after iteration 725: 1.3804297095703346\n",
      "Cost after iteration 750: 1.3777816045588411\n",
      "Cost after iteration 775: 1.3786532362019681\n",
      "Cost after iteration 800: 1.3793948975479549\n",
      "Cost after iteration 825: 1.3809429397852895\n",
      "Cost after iteration 850: 1.3802512098838564\n",
      "Cost after iteration 875: 1.3794316788104515\n",
      "Cost after iteration 900: 1.3741661945281594\n",
      "Cost after iteration 925: 1.3789087957563757\n",
      "Cost after iteration 950: 1.3753088333395482\n",
      "Cost after iteration 975: 1.3788575796002132\n",
      "Cost after iteration 1000: 1.384413343393569\n",
      "Cost after iteration 1025: 1.3786660122139922\n",
      "Cost after iteration 1050: 1.3796098130157746\n",
      "Cost after iteration 1075: 1.3802111114544853\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-190-eb5601faada4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlayer_dims\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlayer_dims\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2001\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00045\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprint_cost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-189-9af97d587726>\u001b[0m in \u001b[0;36mnn\u001b[1;34m(X, Y, layer_dims, num_iters, learning_rate, print_cost)\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mdz1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdA1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mz1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mdW1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdz1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mminibatch_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#+ ((lamd/m)*W1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[0mdb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mn\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdz1\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer_dims =  (X_train.shape[0],200,2)\n",
    "parameters = nn(X_train,Y_train,layer_dims,num_iters=2001,learning_rate=0.00045,print_cost=True)\n",
    "print(parameters.keys())\n",
    "print(len(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def mnist(data):\n",
    "    f = open(data)\n",
    "    reader = csv.reader(f)\n",
    "    i=0 \n",
    "    lst=[]\n",
    "    for read in reader:\n",
    "\n",
    "\n",
    "        im = np.array(read,dtype=float)\n",
    "\n",
    "        im=im.reshape(1,im.shape[0])\n",
    "\n",
    "        lst.append(im)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    X=np.array(lst)\n",
    "\n",
    "    X=X.reshape(X.shape[0],X.shape[1]*X.shape[2])\n",
    "    X=X.T\n",
    "    \n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open(\"al/train.csv\")\n",
    "reader = csv.reader(f)\n",
    "i=0 \n",
    "lst=[]\n",
    "for read in reader:\n",
    "    \n",
    "        \n",
    "    im = np.array(read,dtype=float)\n",
    "    y=int(im[0])\n",
    "    im=im[1:im.shape[0]]\n",
    "    im=im.reshape(im.shape[0],1)\n",
    "   \n",
    "    lst.append([im,y])\n",
    "\n",
    "X=[]\n",
    "Y=[]  \n",
    "for x,y in lst:\n",
    "    X.append(x)\n",
    "    Y.append(y)\n",
    "    \n",
    "X_m=np.array(X)\n",
    "X_m=X_m.reshape(X_m.shape[1]*X_m.shape[2],X_m.shape[0])\n",
    "Y_m=np.array(Y)\n",
    "Y_m=Y_m.reshape(1,Y_m.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_m=X_m/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LAPY\\Desktop\n",
      "(784, 60000)\n"
     ]
    }
   ],
   "source": [
    "#os.chdir(\"../../../\")\n",
    "print(os.getcwd())\n",
    "mnist_X=mnist(\"mnist/train_images_mnist.csv\")\n",
    "print(mnist_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60000)\n"
     ]
    }
   ],
   "source": [
    "mnist_Y=mnist(\"mnist/train_labels_mnist.csv\")\n",
    "print(mnist_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10000)\n",
      "(1, 10000)\n"
     ]
    }
   ],
   "source": [
    "mnist_test_X = mnist(\"mnist/test_images_mnist.csv\")\n",
    "print(mnist_test_X.shape)\n",
    "mnist_test_Y = mnist(\"mnist/test_labels_mnist.csv\")\n",
    "print(mnist_test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 42000)\n"
     ]
    }
   ],
   "source": [
    "print(X_m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 1.3845714494932548\n",
      "Cost after iteration 50: 1.3922784568311228\n",
      "Cost after iteration 100: 1.3923293699932775\n",
      "Cost after iteration 150: 1.3922650148135083\n",
      "Cost after iteration 200: 1.3922019683133884\n",
      "Cost after iteration 250: 1.39214440700892\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['W1', 'b1', 'W2', 'b2', 'W3', 'b3', 'W4', 'b4'])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-1d7e52b92455>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(X,Y,parameters):\n",
    "    z1 = np.dot(parameters[\"W1\"],X) + parameters[\"b1\"]\n",
    "        \n",
    "    A1,_ = relu(z1)\n",
    "        \n",
    "    z2 = np.dot(parameters[\"W2\"],A1) + parameters[\"b2\"]\n",
    "        \n",
    "    problty,_ = sigmoid(z2) \n",
    "    m = X.shape[1]\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "   \n",
    "    \n",
    "    #p = np.where(problty>0.5,1,0)\n",
    "    p=np.argmax(problty,axis=0)+1\n",
    "    print(\"prediction accuracy\" + \" \" + str(np.sum((p == Y))/m))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2022)\n"
     ]
    }
   ],
   "source": [
    "z1 = np.dot(parameters[\"W1\"],X_train) + parameters[\"b1\"]\n",
    "        \n",
    "A1,_ = relu(z1)\n",
    "        \n",
    "z2 = np.dot(parameters[\"W2\"],A1) + parameters[\"b2\"]\n",
    "        \n",
    "problty,_ = sigmoid(z2) \n",
    "print(problty.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 1. ... 7. 6. 9.]]\n"
     ]
    }
   ],
   "source": [
    "Y==np.array(Y,dtype=int)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int32'>\n"
     ]
    }
   ],
   "source": [
    "print(type(Y[0,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]] (1, 2)\n",
      "() 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lapy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "a = np.array(([1,2,1,1,2,1],)).T\n",
    "b= np.array(([1,2],))\n",
    "print(a,b.shape)\n",
    "c=np.where(b==Y,1,0)\n",
    "print(c.T.shape,c.T)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 8005) (1, 8005)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy 0.5487820112429731\n"
     ]
    }
   ],
   "source": [
    "p=prediction(X_train,Y_train,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction accuracy 0.5383094414236282\n"
     ]
    }
   ],
   "source": [
    "p = prediction(X_test,Y_test,parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2 1 ... 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 1 2 2]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[ 1.62434536 ,-0.61175641, -0.52817175],\n",
    " [-1.07296862,  0.86540763, -2.3015387 ],\n",
    " [ 1.74481176 ,-0.7612069  , 0.3190391 ],\n",
    " [-0.24937038 , 1.46210794 ,-2.06014071]])\n",
    "y = np.array([[1, 1, 0]])\n",
    "prm = {'W1': np.array([[-0.3224172 , -0.38405435,  1.13376944, -1.09989127],\n",
    "       [-0.17242821, -0.87785842,  0.04221375,  0.58281521],\n",
    "       [-1.10061918,  1.14472371,  0.90159072,  0.50249434],\n",
    "       [ 0.90085595, -0.68372786, -0.12289023, -0.93576943],\n",
    "       [-0.26788808,  0.53035547, -0.69166075, -0.39675353]]), 'b1': np.array([[-0.6871727 ],\n",
    "       [-0.84520564],\n",
    "       [-0.67124613],\n",
    "       [-0.0126646 ],\n",
    "       [-1.11731035]]), 'W2': np.array([[ 0.2344157 ,  1.65980218,  0.74204416, -0.19183555, -0.88762896],\n",
    "       [-0.74715829,  1.6924546 ,  0.05080775, -0.63699565,  0.19091548],\n",
    "       [ 2.10025514,  0.12015895,  0.61720311,  0.30017032, -0.35224985]]), 'b2': np.array([[-1.1425182 ],\n",
    "       [-0.34934272],\n",
    "       [-0.20889423]]), 'W3': np.array([[ 0.58662319,  0.83898341,  0.93110208]]), 'b3': np.array([[ 0.28558733]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! difference = 7.045356398327675e-08\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "al,caches = forward_propagation(x,prm)\n",
    "gradients = backward_propogation(al,y,caches)\n",
    "d = gradient_checking(prm,gradients,x,y,epsilon = 1e-7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-7690b296ce6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"s.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#X_t=np.array()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mX_t\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mX_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mnist' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "X_t = mnist(\"s.csv\")\n",
    "\n",
    "#X_t=np.array()\n",
    "X_t=X_t.reshape(X_t.shape[0]*X_t.shape[1],1) \n",
    "#print(X_t.shape)\n",
    "#X_t=X_t.reshape(X_t.shape[0], -1).T\n",
    "#print(X_t.shape)   \n",
    "X_t = X_t/255\n",
    "p,cache = forward_propagation(X_t,parameters)\n",
    "pr = np.argmax(p,axis=0)\n",
    "print(p,pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.11855978]\n",
      " [ 0.05148968]\n",
      " [-1.20090158]\n",
      " [ 0.09191999]]\n"
     ]
    }
   ],
   "source": [
    "o = np.random.randn(4,1)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11855978]\n",
      " [0.05148968]\n",
      " [3.        ]\n",
      " [0.09191999]]\n"
     ]
    }
   ],
   "source": [
    "o[2]=3\n",
    "print(o)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['W1', 'b1', 'W2', 'b2'])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(parameters.keys())\n",
    "print(len(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8f9402af70af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
